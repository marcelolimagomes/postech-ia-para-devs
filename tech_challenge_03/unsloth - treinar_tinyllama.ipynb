{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIAP - Curso IA para Devs\n",
    "# Tech Challenge 03 \n",
    "# Problema: \n",
    "#  -- No Tech Challenge desta fase, você precisa executar o fine-tuning de um \n",
    "#  -- foundation model (Llama, BERT, MISTRAL etc.), utilizando o dataset \"The\n",
    "#  -- AmazonTitles-1.3MM\". O modelo treinado deverá:\n",
    "#\n",
    "# Grupo 44\n",
    "# Francisco Antonio Guilherme\n",
    "# fagn2013@gmail.com\n",
    "\n",
    "# Marcelo Lima Gomes\n",
    "# marcelolimagomes@gmail.com\n",
    "\n",
    "# FELIPE MORAES DOS SANTOS\n",
    "# felipe.moraes.santos2014@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente configurado para treinamento local em um PC com Placa de Vídeo Nvidia RTX-3060 12GB\n",
    "\n",
    "# Utilizando miniconda, instalado em um Linux Ubuntu conforme orientações do link: https://docs.anaconda.com/miniconda/\n",
    "# Utilizando miniconda para criação do ambiente do unsloth conforme orientação no link: https://docs.unsloth.ai/get-started/installation/conda-install\n",
    "\n",
    "# >> Para configurar o ambiente, remova o comentário (\"#\") das linhas abaixo e execute os comandos.\n",
    "# Lembre-se de instalar o miniconda previamente.\n",
    "\n",
    "#!pip install nbformat\n",
    "#!conda install -c conda-forge ipywidgets\n",
    "#!conda create --name unsloth_env python=3.10 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "#!conda activate unsloth_env\n",
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse notebook tem como objetivo realizar o treinamento do modelo utilizando do dataset tratado anteriormente\n",
    "\n",
    "import helper # Biblioteca local para apoio ao desenvolvimento\n",
    "import torch  # Biblioteca fundamental para deep learning, usada para criar e treinar modelos de redes neurais. \n",
    "import datasets  # Biblioteca para carregar e preparar conjuntos de dados de diferentes formatos para treinamento de modelos.\n",
    "from trl import SFTTrainer  # Classe para treinar modelos de linguagem de forma supervisionada, ajustando um modelo pré-treinado a uma tarefa específica.\n",
    "from transformers import TrainingArguments  # Define os argumentos de treinamento, como número de épocas, tamanho do lote, etc.\n",
    "from unsloth import is_bfloat16_supported  # Função para verificar se a GPU suporta o formato de ponto flutuante bfloat16, que pode acelerar o treinamento.\n",
    "from unsloth import FastLanguageModel  # Classe para criar modelos de linguagem otimizados para maior velocidade e eficiência.\n",
    "\n",
    "print(\"Versão PyTorch:\", torch.__version__)\n",
    "print(\"Versão CuDa:\", torch.version.cuda)\n",
    "print(\"Suporta Precisão Float 16 bits:\", is_bfloat16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048  # Escolha qualquer um! Nós damos suporte automático ao RoPE Scaling internamente!\n",
    "dtype = None  # None para detecção automática. Float16 para Tesla T4, V100, Bfloat16 para Ampere+\n",
    "load_in_4bit = False  # Use quantização de 4 bits para reduzir o uso de memória. Pode ser Falso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## >>>>> Renomear modelos de entrada e saída de acordo com o ciclo de treinos. Modelos de saída salvos localmente\n",
    "\n",
    "model_name = 'unsloth/tinyllama' ## Modelo de entrada: 1o Ciclo de Treino\n",
    "# model_name = 'marcelolimagomes/tinyllama_finetuned' ## Modelo de entrada: 2o Ciclo de Treino\n",
    "\n",
    "# Nome do modelo a ser salvo\n",
    "save_name = 'marcelolimagomes/tinyllama_finetuned' ## Modelo de saída: 1o Ciclo de Treino\n",
    "# save_name = 'tinyllama_finetuned_2' ## Modelo de saída: 2o Ciclo de Treino\n",
    "\n",
    "raw_model, tokenizer = helper.get_model_by_name(model_name, max_seq_length, dtype, load_in_4bit)  # Carrega modelo na memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt no padrão Alpaca é, em essência, uma forma estruturada de instrução que você fornece a um modelo de linguagem \n",
    "# para direcionar sua resposta. Essa estrutura é projetada para maximizar a qualidade e a relevância das respostas geradas pelo modelo, \n",
    "# tornando as interações mais naturais e informativas.\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a book review.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Marcador que indica o fim de uma sequência de tokens\n",
    "\n",
    "#   Função para formatar o dataset em um conjunto de prompts para serem imputados na sequência de fine tuning do modelo\n",
    "#   examples: Dataset contendo dados para serem treinados\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs       = examples[\"title\"]\n",
    "    outputs      = examples[\"content\"]\n",
    "    texts = []\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # É necessário adicionar EOS_TOKEN, caso contrário sua geração continuará para sempre!\n",
    "        text = alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "# Carrega um dataset préviamente processado, onde foi aplicado a limpeza dos dados.\n",
    "# Esse processo é executado no notebook \"limpeza_dataset.ipynb\"\n",
    "dataset = datasets.Dataset.from_csv('../data/trn_sample.csv', sep=';')\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True,)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define e carrega as preferências para treinamento do modelo informado\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    # Contem o modelo carregado préviamente em memória\n",
    "    raw_model,\n",
    "    # Classificação da decomposição de baixa classificação para fatoração de matrizes de peso. Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    r=16,  \n",
    "    # Seleciona os módulos para fazer o ajuste fino. Você pode remover alguns para reduzir o uso de memória e tornar o treinamento \n",
    "    # mais rápido, mas não sugerimos isso. Apenas treine em todos os módulos!\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # Fator de escala para a contribuição das matrizes de baixa classificação.\n",
    "    lora_alpha=16,\n",
    "    # Probabilidade de zerar elementos em matrizes de baixa classificação para regularização.\n",
    "    lora_dropout=0,  # Suporta any, mas = 0 é otimizado\n",
    "    # Deixe como 0 para um treino mais rápido e com menos over-fit!\n",
    "    bias=\"none\",    # Suporta any, mas = \"none\" é otimizado\n",
    "    # [NEW] \"unsloth\" usa 30% menos VRAM e se adapta a tamanhos de lote 2x maiores!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    # Ativa o Rank-Stabilized LoRA (RSLora).\n",
    "    use_rslora=True,  \n",
    "    # Configuração para LoftQ, um método de quantização para os pesos do backbone e inicialização de camadas LoRA.\n",
    "    loftq_config=None,  \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Definição do Trainer para o modelo supervisionado SFT (Supervised Fine-Tuning)\n",
    "trainer = SFTTrainer(\n",
    "    # O modelo que será treinado\n",
    "    model=model,\n",
    "    # O tokenizador que será utilizado para preparar o texto para o modelo\n",
    "    tokenizer=tokenizer,\n",
    "    # O dataset que será utilizado no processo de treinamento\n",
    "    train_dataset=dataset,\n",
    "    # O campo do dataset que contém o texto de entrada\n",
    "    dataset_text_field=\"text\",\n",
    "    # Comprimento máximo das sequências de entrada\n",
    "    max_seq_length=max_seq_length,\n",
    "    # Número de processos paralelos para pré-processar os dados\n",
    "    dataset_num_proc=2,\n",
    "    # O packing pode ser usado para acelerar o treinamento em até 5x para sequências curtas, porém está desativado\n",
    "    packing=False,\n",
    "    # Argumentos relacionados ao treinamento, configurados usando a classe `TrainingArguments`\n",
    "    args=TrainingArguments(\n",
    "        # Tamanho do lote (batch) de treinamento por dispositivo (GPU ou CPU)\n",
    "        per_device_train_batch_size=128,\n",
    "        # Número de passos de acumulação de gradiente antes de atualizar os pesos\n",
    "        gradient_accumulation_steps=1,\n",
    "        # Quantidade de passos de aquecimento antes de iniciar o treinamento efetivo\n",
    "        warmup_steps=5,\n",
    "        # Número de épocas (passadas completas pelo dataset) que o modelo será treinado\n",
    "        num_train_epochs=30,\n",
    "        # Caminho onde os resultados e checkpoints do modelo serão salvos\n",
    "        output_dir=save_name,\n",
    "        # Estratégia de salvamento dos checkpoints durante o treinamento, definida para salvar a cada N passos\n",
    "        save_strategy=\"steps\",\n",
    "        # Número de passos entre cada salvamento do modelo\n",
    "        save_steps=10,\n",
    "        # Taxa de aprendizado inicial utilizada no otimizador\n",
    "        learning_rate=2e-4,\n",
    "        # Ativa cálculos em precisão mista com FP16 se o hardware suportar, para melhorar a performance\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        # Usa precisão mista com bf16 (formato bfloat16) se o hardware suportar, geralmente para GPUs mais recentes\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        # Número de passos entre cada registro de logs do treinamento\n",
    "        logging_steps=1,\n",
    "        # Define o otimizador AdamW (Adam com decaimento de peso) com 8 bits, mais eficiente em memória\n",
    "        optim=\"adamw_8bit\",\n",
    "        # Taxa de decaimento de peso para regularização do modelo\n",
    "        weight_decay=0.01,\n",
    "        # Tipo de scheduler de taxa de aprendizado, definido como linear\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        # Semente aleatória para garantir reprodutibilidade do treinamento\n",
    "        seed=3407,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime o dispositivo utilizado para treinamento e a quantidade de memória RAM disponível\n",
    "start_gpu_memory, max_memory = helper.print_start_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia o treinamento do modelo com o trainer configurado e armazena as estatísticas do treinamento na variável `trainer_stats`\n",
    "\n",
    "# trainer.train() é o método que executa o processo de treinamento. Ele utiliza os parâmetros definidos no SFTTrainer, como o dataset, o modelo, e os argumentos de treinamento.\n",
    "# O método retorna um objeto com as estatísticas do treinamento, que podem incluir informações como a perda (loss), tempo de treinamento, e outras métricas relevantes.\n",
    "# As estatísticas do treinamento são armazenadas na variável `trainer_stats` para posterior análise ou salvamento.\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime o dispositivo utilizado para treinamento e a quantidade de memória RAM consumida no processo de treinamento\n",
    "helper.print_final_memory_usage(start_gpu_memory, max_memory, trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo treinado localmente no disco o que permite posterior consumo para realização das inferências e geração de texto\n",
    "model.save_pretrained(save_name)  # Local saving\n",
    "tokenizer.save_pretrained(save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o modelo no padrão GUFF treinado localmente no disco o que permite posterior consumo para realização das inferências e geração de texto\n",
    "# O formato GGUF (um formato de checkpoint) com quantização em f16 (precisão de 16 bits)\n",
    "model.save_pretrained_gguf(save_name + \"f16_2\", tokenizer, quantization_method='f16')  # Local saving"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

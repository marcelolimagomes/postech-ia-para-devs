{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate peft bitsandbytes transformers trl\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing More Dependencies\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"marcelolimagomes/llama3.18B-Fine-tuned_FIAP\"\n",
    "#model_id = \"marcelolimagomes/llama3.18B-Instruct-Fine-tuned_FIAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_prompt = \\\n",
    "# \"\"\"<|im_start|>user\n",
    "# Product name [{}]<|im_end|>\n",
    "# <|im_start|>assistant \n",
    "# Review: {}\n",
    "#\"\"\"\n",
    "\n",
    "train_prompt = \\\n",
    "\"\"\"<|im_start|>user\n",
    "Write a review of book [{}]<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_compute_dtype=\"float16\",\n",
    "      bnb_4bit_use_double_quant=True\n",
    "  )\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id,\n",
    "      quantization_config=bnb_config,\n",
    "      device_map=\"auto\"\n",
    "  )\n",
    "  model.config.use_cache = False\n",
    "  model.config.pretraining_tp = 1\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_prompt(question) -> str:\n",
    "  return train_prompt.format(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input, model, tokenizer):\n",
    "  prompt = formatted_prompt(user_input)\n",
    "  inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "\n",
    "  generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.6,\n",
    "    do_sample=True,\n",
    "    top_k=5,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "\n",
    "  start_time = perf_counter()\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "  outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "  theresponse = (tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  output_time = perf_counter() - start_time\n",
    "  print(f\"Time taken for inference: {round(output_time,2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data_used_to_train.csv', sep=';')\n",
    "for _, row in df.sample(frac=1).head(5).iterrows():\n",
    "  title = row['title']\n",
    "  content = row['content']\n",
    "  print(f\"Product name [{title}]\")\n",
    "  print(f\"Original review:\\n {content}\")\n",
    "  print(f\"Generated review:\")\n",
    "  print('---------------------------------------------------------------')\n",
    "  generate_response(title, model, tokenizer)\n",
    "  print('---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ambiente configurado para treinamento local em um PC com Placa de V√≠deo Nvidia RTX-3060 12GB\n",
    "\n",
    "## Utilizando miniconda, instalado em um Linux Ubuntu conforme orienta√ß√µes do link: https://docs.anaconda.com/miniconda/\n",
    "## Utilizando miniconda para cria√ß√£o do ambiente do unsloth conforme orienta√ß√£o no link: https://docs.unsloth.ai/get-started/installation/conda-install\n",
    "\n",
    "## >> Para configurar o ambiente, remova o coment√°rio (\"##\") e execute os comandos. Lembre-se de instalar o miniconda previamente\n",
    "\n",
    "#!pip install nbformat\n",
    "#!conda install -c conda-forge ipywidgets\n",
    "#!conda create --name unsloth_env python=3.10 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "#!conda activate unsloth_env\n",
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "2.4.1\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import torch \n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id Model: 0 - Model Name: unsloth/Meta-Llama-3.1-8B\n",
      "==((====))==  Unsloth 2024.9: Fast Llama patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3060. Max memory: 11.65 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.4.1. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    }
   ],
   "source": [
    "model_name, raw_model, tokenizer = helper.get_model_by_id(0, max_seq_length, dtype, load_in_4bit)  ## \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func_train(examples):        \n",
    "    inputs       = examples['title']\n",
    "    outputs      = examples['content']\n",
    "    texts = []\n",
    "    #for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "        text = helper.alpaca_prompt.format(input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = helper.get_fast_language_model(raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_csv('../data/trn_sample.csv', sep=';', nrows=100)\n",
    "dataset = dataset.map(formatting_prompts_func_train, batched = True,)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        #max_steps = 60,\n",
    "        #learning_rate = 2e-4,\n",
    "        learning_rate = 2e-5,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3060. Max memory = 11.65 GB.\n",
      "5.984 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "start_gpu_memory, max_memory = helper.print_start_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 24\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595aa35d9b554befa324eac4f0a1fdda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9238, 'grad_norm': 5.993089199066162, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.08}\n",
      "{'loss': 3.0795, 'grad_norm': 3.9729456901550293, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.16}\n",
      "{'loss': 3.0278, 'grad_norm': 3.472102165222168, 'learning_rate': 1.2e-05, 'epoch': 0.24}\n",
      "{'loss': 3.2475, 'grad_norm': 3.2045602798461914, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.32}\n",
      "{'loss': 3.131, 'grad_norm': 3.901815414428711, 'learning_rate': 2e-05, 'epoch': 0.4}\n",
      "{'loss': 2.8624, 'grad_norm': 3.1312434673309326, 'learning_rate': 1.894736842105263e-05, 'epoch': 0.48}\n",
      "{'loss': 2.9373, 'grad_norm': 2.9050920009613037, 'learning_rate': 1.7894736842105264e-05, 'epoch': 0.56}\n",
      "{'loss': 2.867, 'grad_norm': 3.1549599170684814, 'learning_rate': 1.6842105263157896e-05, 'epoch': 0.64}\n",
      "{'loss': 3.0518, 'grad_norm': 3.721040725708008, 'learning_rate': 1.578947368421053e-05, 'epoch': 0.72}\n",
      "{'loss': 3.0033, 'grad_norm': 3.091139554977417, 'learning_rate': 1.4736842105263159e-05, 'epoch': 0.8}\n",
      "{'loss': 2.7652, 'grad_norm': 2.70348858833313, 'learning_rate': 1.3684210526315791e-05, 'epoch': 0.88}\n",
      "{'loss': 2.8289, 'grad_norm': 2.8149797916412354, 'learning_rate': 1.263157894736842e-05, 'epoch': 0.96}\n",
      "{'loss': 2.9143, 'grad_norm': 3.1672613620758057, 'learning_rate': 1.1578947368421053e-05, 'epoch': 1.04}\n",
      "{'loss': 2.7489, 'grad_norm': 3.7655487060546875, 'learning_rate': 1.0526315789473684e-05, 'epoch': 1.12}\n",
      "{'loss': 2.6103, 'grad_norm': 3.562788486480713, 'learning_rate': 9.473684210526315e-06, 'epoch': 1.2}\n",
      "{'loss': 2.5138, 'grad_norm': 2.632641077041626, 'learning_rate': 8.421052631578948e-06, 'epoch': 1.28}\n",
      "{'loss': 2.5765, 'grad_norm': 2.6623518466949463, 'learning_rate': 7.368421052631579e-06, 'epoch': 1.36}\n",
      "{'loss': 2.7908, 'grad_norm': 2.7158052921295166, 'learning_rate': 6.31578947368421e-06, 'epoch': 1.44}\n",
      "{'loss': 2.3752, 'grad_norm': 3.2202680110931396, 'learning_rate': 5.263157894736842e-06, 'epoch': 1.52}\n",
      "{'loss': 2.6527, 'grad_norm': 3.2745275497436523, 'learning_rate': 4.210526315789474e-06, 'epoch': 1.6}\n",
      "{'loss': 2.1141, 'grad_norm': 2.875185489654541, 'learning_rate': 3.157894736842105e-06, 'epoch': 1.68}\n",
      "{'loss': 2.32, 'grad_norm': 2.475999355316162, 'learning_rate': 2.105263157894737e-06, 'epoch': 1.76}\n",
      "{'loss': 2.4511, 'grad_norm': 2.838711738586426, 'learning_rate': 1.0526315789473685e-06, 'epoch': 1.84}\n",
      "{'loss': 2.8673, 'grad_norm': 2.7679638862609863, 'learning_rate': 0.0, 'epoch': 1.92}\n",
      "{'train_runtime': 110.5263, 'train_samples_per_second': 1.81, 'train_steps_per_second': 0.217, 'train_loss': 2.7775160570939383, 'epoch': 1.92}\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.5263 seconds used for training.\n",
      "1.84 minutes used for training.\n",
      "Peak reserved memory = 6.52 GB.\n",
      "Peak reserved memory for training = 0.536 GB.\n",
      "Peak reserved memory % of max memory = 55.966 %.\n",
      "Peak reserved memory for training % of max memory = 4.601 %.\n"
     ]
    }
   ],
   "source": [
    "helper.print_final_memory_usage(start_gpu_memory, max_memory, trainer_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Below is a book that contains a title and description. \n",
      "\n",
      "###INSTRUCTION:\n",
      "Write a resume description that appropriately corresponds to the title.\n",
      "\n",
      "###TITLE:\n",
      "Operation Fortitude The True Story of the Key Spy Operation of WWII That Saved DDay\n",
      "\n",
      "###DESCRIPTION:\n",
      "Operation Fortitude was the most successful deception operation ever pulled off. It convinced the Nazis that the Allies would attack at Pas de Calais, the northernmost part of France, rather than Normandy. This convinced the Germans to keep their best troops and equipment there, away from the beaches where the Allied invasion would occur. The deception was so successful that the Nazis even began to believe that the Allies would land there. This book describes the operation and its many elements, including the creation of a phantom army, the creation of fake radio traffic and a phantom air force, and the creation of a phantom general. It also describes how the operation was\n",
      "\n",
      "\n",
      "<|begin_of_text|>Below is a book that contains a title and description. \n",
      "\n",
      "###INSTRUCTION:\n",
      "Write a resume description that appropriately corresponds to the title.\n",
      "\n",
      "###TITLE:\n",
      "I Miss Mummy The true story of a frightened young girl who is desperate to go home\n",
      "\n",
      "###DESCRIPTION:\n",
      "I Miss Mummy is the story of a young girl who is separated from her mother during the Second World War. It is a moving and poignant account of how a young child copes with the trauma of being separated from her mother.<|end_of_text|>\n",
      "\n",
      "\n",
      "<|begin_of_text|>Below is a book that contains a title and description. \n",
      "\n",
      "###INSTRUCTION:\n",
      "Write a resume description that appropriately corresponds to the title.\n",
      "\n",
      "###TITLE:\n",
      "Secrets of Watercolour Success Collins Artists Studio\n",
      "\n",
      "###DESCRIPTION:\n",
      "Secrets of Watercolour Success Collins Artists Studio is a book that will help you become a successful watercolour artist. It is a book that will show you how to create beautiful watercolour paintings that will impress your friends and family. It is a book that will teach you the secrets of watercolour success. It is a book that will help you become a successful watercolour artist.<|end_of_text|>\n",
      "\n",
      "\n",
      "<|begin_of_text|>Below is a book that contains a title and description. \n",
      "\n",
      "###INSTRUCTION:\n",
      "Write a resume description that appropriately corresponds to the title.\n",
      "\n",
      "###TITLE:\n",
      "The Girl in the Mirror\n",
      "\n",
      "###DESCRIPTION:\n",
      "A young girl is looking at herself in a mirror. She is sitting on a window seat, with her back to the window, her face turned towards the mirror. She is wearing a white dress with a red sash. Her long hair is hanging down her back, and she is holding a bunch of flowers. She is looking at the mirror with a puzzled expression.<|end_of_text|>\n",
      "\n",
      "\n",
      "<|begin_of_text|>Below is a book that contains a title and description. \n",
      "\n",
      "###INSTRUCTION:\n",
      "Write a resume description that appropriately corresponds to the title.\n",
      "\n",
      "###TITLE:\n",
      "A Christmas Promise\n",
      "\n",
      "###DESCRIPTION:\n",
      "A Christmas Promise is a short story by Mary Elizabeth Braddon. It was published in 1856.\n",
      "\n",
      "###RESUME DESCRIPTION:\n",
      "A Christmas Promise is a short story by Mary Elizabeth Braddon. It was published in 1856.<|end_of_text|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Teste do modelo depois do treinamento\n",
    "\n",
    "df = dataset.to_pandas().sample(frac=1).head(5).copy()\n",
    "for _, row in df.iterrows():\n",
    "  title = row['title']\n",
    "  helper.predict_text_streamer(model, tokenizer, title)\n",
    "  print('\\n')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meta-Llama-3.1-8B'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name.split('/')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Meta-Llama-3.1-8B/tokenizer_config.json',\n",
       " 'Meta-Llama-3.1-8B/special_tokens_map.json',\n",
       " 'Meta-Llama-3.1-8B/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(model_name.split('/')[1]) # Local saving\n",
    "tokenizer.save_pretrained(model_name.split('/')[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

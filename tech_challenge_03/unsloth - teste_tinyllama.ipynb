{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIAP - Curso IA para Devs\n",
    "# Tech Challenge 03 \n",
    "# Problema: \n",
    "#  -- No Tech Challenge desta fase, você precisa executar o fine-tuning de um \n",
    "#  -- foundation model (Llama, BERT, MISTRAL etc.), utilizando o dataset \"The\n",
    "#  -- AmazonTitles-1.3MM\". O modelo treinado deverá:\n",
    "#\n",
    "# Grupo 44\n",
    "# Francisco Antonio Guilherme\n",
    "# fagn2013@gmail.com\n",
    "\n",
    "# Marcelo Lima Gomes\n",
    "# marcelolimagomes@gmail.com\n",
    "\n",
    "# FELIPE MORAES DOS SANTOS\n",
    "# felipe.moraes.santos2014@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente configurado para treinamento local em um PC com Placa de Vídeo Nvidia RTX-3060 12GB\n",
    "\n",
    "# Utilizando miniconda, instalado em um Linux Ubuntu conforme orientações do link: https://docs.anaconda.com/miniconda/\n",
    "# Utilizando miniconda para criação do ambiente do unsloth conforme orientação no link: https://docs.unsloth.ai/get-started/installation/conda-install\n",
    "\n",
    "# >> Para configurar o ambiente, remova o comentário (\"#\") das linhas abaixo e execute os comandos.\n",
    "# Lembre-se de instalar o miniconda previamente.\n",
    "\n",
    "#!pip install nbformat\n",
    "#!conda install -c conda-forge ipywidgets\n",
    "#!conda create --name unsloth_env python=3.10 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "#!conda activate unsloth_env\n",
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esse notebook tem como objetivo realizar a geração de texto, que neste caso trata-se da, geração de avaliações de produtos com base em dados\n",
    "# disponibilizados por meio do dataset do exercício.\n",
    "\n",
    "from transformers import TextStreamer\n",
    "import pandas as pd\n",
    "import helper # Biblioteca local para apoio ao desenvolvimento\n",
    "import torch  # Biblioteca fundamental para deep learning, usada para criar e treinar modelos de redes neurais. \n",
    "import datasets  # Biblioteca para carregar e preparar conjuntos de dados de diferentes formatos para treinamento de modelos.\n",
    "from unsloth import is_bfloat16_supported  # Função para verificar se a GPU suporta o formato de ponto flutuante bfloat16, que pode acelerar o treinamento.\n",
    "from unsloth import FastLanguageModel  # Classe para criar modelos de linguagem otimizados para maior velocidade e eficiência.\n",
    "\n",
    "print(\"Versão PyTorch:\", torch.__version__)\n",
    "print(\"Versão CuDa:\", torch.version.cuda)\n",
    "print(\"Suporta Precisão Float 16 bits:\", is_bfloat16_supported())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048  # Escolha qualquer um! Nós damos suporte automático ao RoPE Scaling internamente!\n",
    "dtype = None  # None para detecção automática. Float16 para Tesla T4, V100, Bfloat16 para Ampere+\n",
    "load_in_4bit = False  # Use quantização de 4 bits para reduzir o uso de memória. Pode ser Falso.\n",
    "\n",
    "# Nome do modelo treinado préviamente, esse modelo passou por dois processos de treinamento com 40 épocas de treinamento e +3000 steps, e 10h de processamento.\n",
    "# Somente então o modelo convergiu!\n",
    "model_name = 'tinyllama_finetuned_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model, tokenizer = helper.get_model_by_name(model_name, max_seq_length, dtype, load_in_4bit)  # Carrega modelo na memória"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define e carrega as preferências para treinamento do modelo informado\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    # Contem o modelo carregado préviamente em memória\n",
    "    raw_model,\n",
    "    # Classificação da decomposição de baixa classificação para fatoração de matrizes de peso. Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    r=16,  \n",
    "    # Seleciona os módulos para fazer o ajuste fino. Você pode remover alguns para reduzir o uso de memória e tornar o treinamento \n",
    "    # mais rápido, mas não sugerimos isso. Apenas treine em todos os módulos!\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    # Fator de escala para a contribuição das matrizes de baixa classificação.\n",
    "    lora_alpha=16,\n",
    "    # Probabilidade de zerar elementos em matrizes de baixa classificação para regularização.\n",
    "    lora_dropout=0,  # Suporta any, mas = 0 é otimizado\n",
    "    # Deixe como 0 para um treino mais rápido e com menos over-fit!\n",
    "    bias=\"none\",    # Suporta any, mas = \"none\" é otimizado\n",
    "    # [NEW] \"unsloth\" usa 30% menos VRAM e se adapta a tamanhos de lote 2x maiores!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    # Ativa o Rank-Stabilized LoRA (RSLora).\n",
    "    use_rslora=True,  \n",
    "    # Configuração para LoftQ, um método de quantização para os pesos do backbone e inicialização de camadas LoRA.\n",
    "    loftq_config=None,  \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime o dispositivo utilizado para treinamento e a quantidade de memória RAM disponível\n",
    "start_gpu_memory, max_memory = helper.print_start_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt no padrão Alpaca é, em essência, uma forma estruturada de instrução que você fornece a um modelo de linguagem \n",
    "# para direcionar sua resposta. Essa estrutura é projetada para maximizar a qualidade e a relevância das respostas geradas pelo modelo, \n",
    "# tornando as interações mais naturais e informativas.\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "Write a book review.\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token # Marcador que indica o fim de uma sequência de tokens\n",
    "\n",
    "#   Função que realiza o chamamento das rotinas de geração de texto por inferência com base no parâmetro \"Titulo do produto\" \n",
    "def predict_text_streamer(model, tokenizer, title):\n",
    "  # Ativa a inferência nativa do modelo 2x mais rápido\n",
    "  FastLanguageModel.for_inference(model) \n",
    "  # Converte os parâmetros de entrada (Títulos de Produtos) em prompt e em seguida os converte em tokens a serem enviados ao modelo\n",
    "  inputs = tokenizer([alpaca_prompt.format(title, '')], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "  # Gera o texto em formado te Streaming com base no prompt enviado ao modelo\n",
    "  text_streamer = TextStreamer(tokenizer)\n",
    "  _ = model.generate(**inputs, streamer=text_streamer, max_new_tokens=128, temperature=0.3)\n",
    "\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo depois do treinamento\n",
    "\n",
    "# O Código abaixo carrega na memória o dataset treinado préviamente, de forma aleatória recupera o Título de um produto e o informa para o modelo\n",
    "# para que a avaliação deste respectivo produto seja gerada pelo modelo.\n",
    "# Para fins didáticos, em primeiro momento é impresso o Titulo do produto e sua respectiva avaliação.\n",
    "# Na sequência é impresso o texto da avaliação do produto gerado pelo modelo e assim nos permite comprará-lo com a avaliação original que consta no dataset.\n",
    "\n",
    "df = pd.read_csv('../data/trn_sample.csv', sep=';')\n",
    "for _, row in df.sample(frac=1).head(5).iterrows():\n",
    "  title = row['title']\n",
    "  content = row['content']\n",
    "  print(f\"Título do produto: [{title}]\")\n",
    "  print(f\"Avaliação original:\\n {content}\")\n",
    "  print(f\"Avaliação gerada pelo modelo:\")\n",
    "  print('---------------------------------------------------------------')\n",
    "  predict_text_streamer(model, tokenizer, title)\n",
    "  print('---------------------------------------------------------------')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depois de 40 épocas de treinamento e +3000 steps, o modelo convergiu!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

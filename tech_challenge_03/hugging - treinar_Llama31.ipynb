{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente configurado para treinamento local em um PC com Placa de Vídeo Nvidia RTX-3060 12GB\n",
    "\n",
    "# Utilizando miniconda, instalado em um Linux Ubuntu conforme orientações do link: https://docs.anaconda.com/miniconda/\n",
    "# Utilizando miniconda para criação do ambiente do unsloth conforme orientação no link: https://docs.unsloth.ai/get-started/installation/conda-install\n",
    "\n",
    "# >> Para configurar o ambiente, remova o comentário (\"##\") e execute os comandos. Lembre-se de instalar o miniconda previamente\n",
    "\n",
    "#!pip install nbformat\n",
    "#!conda install -c conda-forge ipywidgets\n",
    "#!conda create --name unsloth_env python=3.10 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "#!conda activate unsloth_env\n",
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "#!pip install accelerate peft bitsandbytes transformers trl\n",
    "\n",
    "# 4o Ciclo de Treinamentonvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing More Dependencies\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id=\"meta-llama/Meta-Llama-3.1-8B\"\n",
    "model_id=\"marcelolimagomes/llama3.18B-Fine-tuned_FIAP_3\"\n",
    "output_model = \"marcelolimagomes/llama3.18B-Fine-tuned_FIAP_4\"\n",
    "\n",
    "#model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "#output_model = \"marcelolimagomes/llama3.18B-Instruct-Fine-tuned_FIAP\"\n",
    "\n",
    "SEED = 123\n",
    "MAX_ROWS = 10000 # Amostra (sample) Dez mil registros\n",
    "\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "SUPPORTS_BFLOAT16 = False\n",
    "HAS_FLASH_ATTENTION = False\n",
    "HAS_FLASH_ATTENTION_SOFTCAPPING = False\n",
    "\n",
    "if major_version >= 8:\n",
    "  SUPPORTS_BFLOAT16 = True\n",
    "\n",
    "# Fixes a weird Torch 2.3 bug which says T4s have bfloat16\n",
    "def is_bfloat16_supported():\n",
    "  return SUPPORTS_BFLOAT16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_prompt = \\\n",
    "# \"\"\"<|im_start|>user\n",
    "# Title of book [{}]<|im_end|>\n",
    "# <|im_start|>assistant \n",
    "# Review of book [{}]: {}<|im_end|>\n",
    "# \"\"\"\n",
    "\n",
    "train_prompt = \\\n",
    "\"\"\"<|im_start|>user\n",
    "Title of book [{}]<|im_end|>\n",
    "<|im_start|>assistant \n",
    "Customers Review of book [{}]: {}<|im_end|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_fix_seed(seed=123):\n",
    "  # Python random\n",
    "  random.seed(seed)\n",
    "  # Numpy\n",
    "  np.random.seed(seed)\n",
    "  # Pytorch\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.use_deterministic_algorithms = True\n",
    "  os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "  # Enable CUDNN deterministic mode\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "#torch_fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_start_memory_usage():\n",
    "  gpu_stats = torch.cuda.get_device_properties(0)\n",
    "  start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "  max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "  print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "  print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "  return start_gpu_memory, max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True, \n",
    "      bnb_4bit_quant_type=\"nf4\", \n",
    "      bnb_4bit_compute_dtype=\"float16\", \n",
    "      bnb_4bit_use_double_quant=True\n",
    "  )\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id, \n",
    "      quantization_config=bnb_config, \n",
    "      device_map=\"auto\"\n",
    "  )\n",
    "  model.config.use_cache=False\n",
    "  model.config.pretraining_tp=1\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_datav2(data_df: pd.DataFrame):\n",
    "  # Create a new column called \"text\"\n",
    "  data_df.to_csv('./data_used_to_train.csv', sep=';', index=False)\n",
    "  data_df[\"text\"] = data_df[[\"title\", \"content\"]].apply(lambda x: train_prompt.format(x[\"title\"], x[\"title\"], x[\"content\"]), axis=1)\n",
    "  # Create a new Dataset from the DataFrame\n",
    "  data = Dataset.from_pandas(data_df)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/trn_sample.csv', sep=';', nrows=MAX_ROWS)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_train_datav2(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)\n",
    "\n",
    "# tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_start_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_model,\n",
    "    # per_device_train_batch_size=4,\n",
    "    # gradient_accumulation_steps=16,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    #save_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=10, \n",
    "    logging_steps=1,\n",
    "    num_train_epochs=2,\n",
    "    #max_steps=250,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    push_to_hub=True,\n",
    "    weight_decay = 0.01,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_start_memory_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

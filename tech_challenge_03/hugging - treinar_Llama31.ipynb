{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiente configurado para treinamento local em um PC com Placa de Vídeo Nvidia RTX-3060 12GB\n",
    "\n",
    "# Utilizando miniconda, instalado em um Linux Ubuntu conforme orientações do link: https://docs.anaconda.com/miniconda/\n",
    "# Utilizando miniconda para criação do ambiente do unsloth conforme orientação no link: https://docs.unsloth.ai/get-started/installation/conda-install\n",
    "\n",
    "# >> Para configurar o ambiente, remova o comentário (\"##\") e execute os comandos. Lembre-se de instalar o miniconda previamente\n",
    "\n",
    "#!pip install nbformat\n",
    "#!conda install -c conda-forge ipywidgets\n",
    "#!conda create --name unsloth_env python=3.10 pytorch-cuda=12.1 pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers -y\n",
    "#!conda activate unsloth_env\n",
    "#!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "#!pip install --no-deps \"trl<0.9.0\" peft accelerate bitsandbytes\n",
    "\n",
    "#!pip install accelerate peft bitsandbytes transformers trl\n",
    "\n",
    "# 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe5afbe00d04dc5889f65d476d02c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing More Dependencies\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "from time import perf_counter\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"unsloth/Meta-Llama-3.1-8B\"\n",
    "\n",
    "SEED = 123\n",
    "MAX_ROWS = 100\n",
    "\n",
    "major_version, minor_version = torch.cuda.get_device_capability()\n",
    "SUPPORTS_BFLOAT16 = False\n",
    "HAS_FLASH_ATTENTION = False\n",
    "HAS_FLASH_ATTENTION_SOFTCAPPING = False\n",
    "\n",
    "if major_version >= 8:\n",
    "  SUPPORTS_BFLOAT16 = True\n",
    "\n",
    "# Fixes a weird Torch 2.3 bug which says T4s have bfloat16\n",
    "\n",
    "\n",
    "def is_bfloat16_supported():\n",
    "  return SUPPORTS_BFLOAT16\n",
    "\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt = \\\n",
    "\"\"\"<|im_start|>user\n",
    "Product name [{}]<|im_end|>\n",
    "<|im_start|>assistant \n",
    "Review: {}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_fix_seed(seed=123):\n",
    "  # Python random\n",
    "  random.seed(seed)\n",
    "  # Numpy\n",
    "  np.random.seed(seed)\n",
    "  # Pytorch\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.use_deterministic_algorithms = True\n",
    "  os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "  os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":16:8\"\n",
    "  # Enable CUDNN deterministic mode\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "torch_fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_start_memory_usage():\n",
    "  gpu_stats = torch.cuda.get_device_properties(0)\n",
    "  start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "  max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "  print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "  print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "  return start_gpu_memory, max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(model_id):\n",
    "  tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True, \n",
    "      bnb_4bit_quant_type=\"nf4\", \n",
    "      bnb_4bit_compute_dtype=\"float16\", \n",
    "      bnb_4bit_use_double_quant=True\n",
    "  )\n",
    "  model = AutoModelForCausalLM.from_pretrained(\n",
    "      model_id, \n",
    "      quantization_config=bnb_config, \n",
    "      device_map=\"auto\"\n",
    "  )\n",
    "  model.config.use_cache=False\n",
    "  model.config.pretraining_tp=1\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_prompt(question) -> str:\n",
    "  return train_prompt.format(question, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_input, model, tokenizer):\n",
    "  prompt = formatted_prompt(user_input)\n",
    "  inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "  \n",
    "  generation_config = GenerationConfig(\n",
    "    penalty_alpha=0.6,\n",
    "    do_sample = True,\n",
    "    top_k=5,\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.2,\n",
    "    max_new_tokens=100,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    "  )\n",
    "  \n",
    "  start_time = perf_counter()\n",
    "  inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "  outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "  theresponse = (tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  output_time = perf_counter() - start_time\n",
    "  print(f\"Time taken for inference: {round(output_time,2)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_datav2(data_df: pd.DataFrame):\n",
    "  # Create a new column called \"text\"\n",
    "  data_df.to_csv('./data_used_to_train.csv', sep=';', index=False)\n",
    "  data_df[\"text\"] = data_df[[\"title\", \"content\"]].apply(lambda x: train_prompt.format(x[\"title\"], x[\"content\"]), axis=1)\n",
    "  # Create a new Dataset from the DataFrame\n",
    "  data = Dataset.from_pandas(data_df)\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   title    100 non-null    object\n",
      " 1   content  100 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../data/trn_sample.csv', sep=';', nrows=MAX_ROWS)\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_train_datav2(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'content', 'text'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4ec0f709d94f67912531248c601781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(model_id)\n",
    "\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3060. Max memory = 11.65 GB.\n",
      "5.508 GB of memory reserved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.508, 11.65)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_start_memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Int: 7\n",
      "Random Title: [Castle Cats]\n"
     ]
    }
   ],
   "source": [
    "int_randon = random.randint(1, MAX_ROWS)\n",
    "print(f'Random Int: {int_randon}')\n",
    "title_predict = dataset.sample(frac=1, random_state=SEED).head(int_randon)['title'].values[0]\n",
    "print(f'Random Title: [{title_predict}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Product name [Castle Cats]<|im_end|>\n",
      "<|im_start|>assistant \n",
      "Review: \n",
      "The game is a great time killer, but it's not very challenging. I'm on level 10 and have only lost once.\n",
      "I like the graphics and how you can customize your cat with different hats etc. It would be nice if there were more options for customizing though (like changing fur color or adding accessories).\n",
      "There are also some issues that need to be fixed:\n",
      "- The sound effects aren't always accurate; sometimes they play when something happens off-screen instead of what actually happened in-game\n",
      "Time taken for inference: 6.5 seconds\n"
     ]
    }
   ],
   "source": [
    "generate_response(title_predict, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = \"llama3.18B-Fine-tuned_FIAP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_model,\n",
    "    # per_device_train_batch_size=4,\n",
    "    # gradient_accumulation_steps=16,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=1,\n",
    "    num_train_epochs=6,\n",
    "    #max_steps=250,\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    push_to_hub=True,\n",
    "    weight_decay = 0.01,\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f5744637704912b0a185c11c45ceba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    args=training_arguments,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=False,\n",
    "    max_seq_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac5ec8deb0d4773b7167d7599a8696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/72 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8427, 'grad_norm': 1.09375, 'learning_rate': 0.0001999048221581858, 'epoch': 0.08}\n",
      "{'loss': 3.023, 'grad_norm': 0.8046875, 'learning_rate': 0.00019961946980917456, 'epoch': 0.16}\n",
      "{'loss': 2.9056, 'grad_norm': 0.86328125, 'learning_rate': 0.00019914448613738106, 'epoch': 0.24}\n",
      "{'loss': 2.9385, 'grad_norm': 0.94921875, 'learning_rate': 0.00019848077530122083, 'epoch': 0.32}\n",
      "{'loss': 2.8924, 'grad_norm': 1.0, 'learning_rate': 0.00019762960071199333, 'epoch': 0.4}\n",
      "{'loss': 2.789, 'grad_norm': 1.2421875, 'learning_rate': 0.00019659258262890683, 'epoch': 0.48}\n",
      "{'loss': 2.6178, 'grad_norm': 1.3046875, 'learning_rate': 0.0001953716950748227, 'epoch': 0.56}\n",
      "{'loss': 2.3835, 'grad_norm': 1.6484375, 'learning_rate': 0.00019396926207859084, 'epoch': 0.64}\n",
      "{'loss': 2.6041, 'grad_norm': 1.984375, 'learning_rate': 0.0001923879532511287, 'epoch': 0.72}\n",
      "{'loss': 2.2032, 'grad_norm': 3.515625, 'learning_rate': 0.000190630778703665, 'epoch': 0.8}\n",
      "{'loss': 1.9876, 'grad_norm': 3.515625, 'learning_rate': 0.00018870108331782217, 'epoch': 0.88}\n",
      "{'loss': 2.354, 'grad_norm': 4.34375, 'learning_rate': 0.00018660254037844388, 'epoch': 0.96}\n",
      "{'loss': 2.6469, 'grad_norm': 5.75, 'learning_rate': 0.0001843391445812886, 'epoch': 1.04}\n",
      "{'loss': 2.2373, 'grad_norm': 7.59375, 'learning_rate': 0.0001819152044288992, 'epoch': 1.12}\n",
      "{'loss': 2.075, 'grad_norm': 4.75, 'learning_rate': 0.00017933533402912354, 'epoch': 1.2}\n",
      "{'loss': 1.6876, 'grad_norm': 2.578125, 'learning_rate': 0.0001766044443118978, 'epoch': 1.28}\n",
      "{'loss': 1.7779, 'grad_norm': 1.890625, 'learning_rate': 0.0001737277336810124, 'epoch': 1.36}\n",
      "{'loss': 1.928, 'grad_norm': 1.984375, 'learning_rate': 0.00017071067811865476, 'epoch': 1.44}\n",
      "{'loss': 1.8134, 'grad_norm': 1.9765625, 'learning_rate': 0.00016755902076156604, 'epoch': 1.52}\n",
      "{'loss': 2.0778, 'grad_norm': 1.71875, 'learning_rate': 0.00016427876096865394, 'epoch': 1.6}\n",
      "{'loss': 1.6099, 'grad_norm': 1.6484375, 'learning_rate': 0.00016087614290087208, 'epoch': 1.68}\n",
      "{'loss': 1.9481, 'grad_norm': 1.3671875, 'learning_rate': 0.0001573576436351046, 'epoch': 1.76}\n",
      "{'loss': 1.6842, 'grad_norm': 1.2890625, 'learning_rate': 0.0001537299608346824, 'epoch': 1.84}\n",
      "{'loss': 1.8659, 'grad_norm': 1.15625, 'learning_rate': 0.00015000000000000001, 'epoch': 1.92}\n",
      "{'loss': 1.9838, 'grad_norm': 1.0703125, 'learning_rate': 0.00014617486132350343, 'epoch': 2.0}\n",
      "{'loss': 1.7557, 'grad_norm': 0.97265625, 'learning_rate': 0.00014226182617406996, 'epoch': 2.08}\n",
      "{'loss': 1.835, 'grad_norm': 1.3046875, 'learning_rate': 0.000138268343236509, 'epoch': 2.16}\n",
      "{'loss': 1.772, 'grad_norm': 0.953125, 'learning_rate': 0.00013420201433256689, 'epoch': 2.24}\n",
      "{'loss': 1.7786, 'grad_norm': 1.375, 'learning_rate': 0.00013007057995042732, 'epoch': 2.32}\n",
      "{'loss': 2.111, 'grad_norm': 1.1015625, 'learning_rate': 0.00012588190451025207, 'epoch': 2.4}\n",
      "{'loss': 1.7507, 'grad_norm': 1.1953125, 'learning_rate': 0.00012164396139381029, 'epoch': 2.48}\n",
      "{'loss': 1.8997, 'grad_norm': 2.15625, 'learning_rate': 0.00011736481776669306, 'epoch': 2.56}\n",
      "{'loss': 1.5679, 'grad_norm': 1.0859375, 'learning_rate': 0.00011305261922200519, 'epoch': 2.64}\n",
      "{'loss': 1.4129, 'grad_norm': 1.2890625, 'learning_rate': 0.00010871557427476583, 'epoch': 2.72}\n",
      "{'loss': 1.5037, 'grad_norm': 1.109375, 'learning_rate': 0.00010436193873653361, 'epoch': 2.8}\n",
      "{'loss': 1.7097, 'grad_norm': 1.1328125, 'learning_rate': 0.0001, 'epoch': 2.88}\n",
      "{'loss': 1.8378, 'grad_norm': 1.125, 'learning_rate': 9.563806126346642e-05, 'epoch': 2.96}\n",
      "{'loss': 1.9512, 'grad_norm': 0.8984375, 'learning_rate': 9.128442572523417e-05, 'epoch': 3.04}\n",
      "{'loss': 1.8518, 'grad_norm': 1.09375, 'learning_rate': 8.694738077799488e-05, 'epoch': 3.12}\n",
      "{'loss': 1.7615, 'grad_norm': 1.078125, 'learning_rate': 8.263518223330697e-05, 'epoch': 3.2}\n",
      "{'loss': 1.6361, 'grad_norm': 1.1015625, 'learning_rate': 7.835603860618972e-05, 'epoch': 3.28}\n",
      "{'loss': 1.4924, 'grad_norm': 1.109375, 'learning_rate': 7.411809548974792e-05, 'epoch': 3.36}\n",
      "{'loss': 1.6681, 'grad_norm': 1.515625, 'learning_rate': 6.992942004957271e-05, 'epoch': 3.44}\n",
      "{'loss': 1.4253, 'grad_norm': 0.97265625, 'learning_rate': 6.579798566743314e-05, 'epoch': 3.52}\n",
      "{'loss': 1.5642, 'grad_norm': 0.90625, 'learning_rate': 6.173165676349103e-05, 'epoch': 3.6}\n",
      "{'loss': 1.2817, 'grad_norm': 1.1015625, 'learning_rate': 5.773817382593008e-05, 'epoch': 3.68}\n",
      "{'loss': 1.9055, 'grad_norm': 1.1484375, 'learning_rate': 5.382513867649663e-05, 'epoch': 3.76}\n",
      "{'loss': 1.6304, 'grad_norm': 1.2421875, 'learning_rate': 5.000000000000002e-05, 'epoch': 3.84}\n",
      "{'loss': 1.8985, 'grad_norm': 1.5390625, 'learning_rate': 4.6270039165317605e-05, 'epoch': 3.92}\n",
      "{'loss': 2.0661, 'grad_norm': 1.2578125, 'learning_rate': 4.264235636489542e-05, 'epoch': 4.0}\n",
      "{'loss': 1.6214, 'grad_norm': 1.171875, 'learning_rate': 3.9123857099127936e-05, 'epoch': 4.08}\n",
      "{'loss': 1.7088, 'grad_norm': 1.5625, 'learning_rate': 3.5721239031346066e-05, 'epoch': 4.16}\n",
      "{'loss': 1.5704, 'grad_norm': 1.1796875, 'learning_rate': 3.244097923843398e-05, 'epoch': 4.24}\n",
      "{'loss': 1.628, 'grad_norm': 1.2734375, 'learning_rate': 2.9289321881345254e-05, 'epoch': 4.32}\n",
      "{'loss': 1.6988, 'grad_norm': 1.1953125, 'learning_rate': 2.6272266318987603e-05, 'epoch': 4.4}\n",
      "{'loss': 1.6541, 'grad_norm': 1.109375, 'learning_rate': 2.339555568810221e-05, 'epoch': 4.48}\n",
      "{'loss': 1.2406, 'grad_norm': 1.046875, 'learning_rate': 2.0664665970876496e-05, 'epoch': 4.56}\n",
      "{'loss': 2.1695, 'grad_norm': 1.0390625, 'learning_rate': 1.808479557110081e-05, 'epoch': 4.64}\n",
      "{'loss': 1.7459, 'grad_norm': 1.1640625, 'learning_rate': 1.566085541871145e-05, 'epoch': 4.72}\n",
      "{'loss': 1.8534, 'grad_norm': 1.390625, 'learning_rate': 1.339745962155613e-05, 'epoch': 4.8}\n",
      "{'loss': 1.6715, 'grad_norm': 1.046875, 'learning_rate': 1.129891668217783e-05, 'epoch': 4.88}\n",
      "{'loss': 1.1435, 'grad_norm': 1.15625, 'learning_rate': 9.369221296335006e-06, 'epoch': 4.96}\n",
      "{'loss': 1.6015, 'grad_norm': 1.0859375, 'learning_rate': 7.612046748871327e-06, 'epoch': 5.04}\n",
      "{'loss': 1.6704, 'grad_norm': 1.1171875, 'learning_rate': 6.030737921409169e-06, 'epoch': 5.12}\n",
      "{'loss': 1.4879, 'grad_norm': 1.078125, 'learning_rate': 4.628304925177318e-06, 'epoch': 5.2}\n",
      "{'loss': 1.3957, 'grad_norm': 1.1328125, 'learning_rate': 3.40741737109318e-06, 'epoch': 5.28}\n",
      "{'loss': 1.8606, 'grad_norm': 1.2265625, 'learning_rate': 2.3703992880066638e-06, 'epoch': 5.36}\n",
      "{'loss': 2.1121, 'grad_norm': 1.0390625, 'learning_rate': 1.5192246987791981e-06, 'epoch': 5.44}\n",
      "{'loss': 1.7028, 'grad_norm': 1.4375, 'learning_rate': 8.555138626189618e-07, 'epoch': 5.52}\n",
      "{'loss': 1.4739, 'grad_norm': 1.078125, 'learning_rate': 3.805301908254455e-07, 'epoch': 5.6}\n",
      "{'loss': 1.431, 'grad_norm': 1.1875, 'learning_rate': 9.517784181422019e-08, 'epoch': 5.68}\n",
      "{'loss': 1.9003, 'grad_norm': 1.1953125, 'learning_rate': 0.0, 'epoch': 5.76}\n",
      "{'train_runtime': 354.6898, 'train_samples_per_second': 1.692, 'train_steps_per_second': 0.203, 'train_loss': 1.8928722540537517, 'epoch': 5.76}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=72, training_loss=1.8928722540537517, metrics={'train_runtime': 354.6898, 'train_samples_per_second': 1.692, 'train_steps_per_second': 0.203, 'total_flos': 3636165317738496.0, 'train_loss': 1.8928722540537517, 'epoch': 5.76})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3060. Max memory = 11.65 GB.\n",
      "9.738 GB of memory reserved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.738, 11.65)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_start_memory_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

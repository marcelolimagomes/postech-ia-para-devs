{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; \n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helper.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model, tokenizer = get_model_by_id(0)  ## \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
    "def formatting_prompts_func_test(examples):  \n",
    "  inputs       = examples['title']\n",
    "  texts = []\n",
    "  #for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "  for input in zip(inputs):\n",
    "      # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
    "      text = alpaca_prompt.format(input, '') + EOS_TOKEN\n",
    "      texts.append(text)\n",
    "  return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_fast_language_model(raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_csv('../data/trn_sample.csv', sep=';')\n",
    "dataset = dataset.map(formatting_prompts_func_test, batched = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do modelo antes do treinamento.\n",
    "df = dataset.to_pandas().sample(frac=1, random_state=42).head(5).copy()\n",
    "for _, row in df.iterrows():\n",
    "  title = row['title']\n",
    "  res = predict_text_streamer(raw_model, tokenizer, title)\n",
    "  print(f\"Resultado da predição para o título: [{title}]\\n\", res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
